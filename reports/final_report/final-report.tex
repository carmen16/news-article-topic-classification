\documentclass[11pt,a4paper]{article}

\usepackage[nohyperref]{acl2018} % references acl2018.sty
%\usepackage{cite}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{graphicx}
%\usepackage{pdflscape}
\usepackage{url}

\aclfinalcopy % Uncomment this line to show the author and remove the line numbers on the sides

\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Optimal Model Input for Newspaper Topic Classification}

\author{Carmen Easterwood \\
  W266 Natural Language Processing with Deep Learning \\
  Summer 2018 \\
}

\date{August 10, 2018}

%%%%%%%%%%%%%%%
% End header information
%%%%%%%%%%%%%%%

\graphicspath{ {../../plots/} }

\begin{document}
\maketitle

\begin{abstract}
Topic classification is an important method of organizing text content and making it more easily searchable, but in the newspaper setting the question remains: what exactly should we be feeding into topic classification models? In this paper, I test five different model inputs: full text, lead paragraph, headline, nouns-only version of the full text, and lemmatized full text. In general I find that inputs with more words perform better, but that a larger vocabulary isn't necessarily better. While nouns make up only 27\% of the words and 61\% of the vocabulary in a newspaper article, they perform almost as well as the full text, making them possibly the most efficient model input.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Topic classification is an important form of content analysis that has been widely studied in the NLP community. Tagging documents by topic makes searches more efficient, so users can find the information they're looking for more quickly and without having to sort through unrelated documents. Unfortunately, manual topic-coding on a large scale is quite time-consuming, so automating this process can save both time and money for organizations with a large amount of text data.

In this paper, I will look at topic classification specifically in the newspaper context. In addition to making searches more efficient for news consumers and researchers, classifying news articles by topic also helps us understand what journalists are writing about, whether that is changing over time, and the relative importance of different topics. Since many news organizations have seen declining revenue over the last decade \cite{Pew}, a cost-saving measure like automation of topic-coding would likely be welcome. However, in the existing literature there is no agreement on what specifically should be fed into topic classification models to get the best results. In this paper, I develop three supervised topic classification models and test several possible model inputs drawn from New York Times articles. For each model, my main focus is how the model inputs perform relative to each other, and which input gives the best result in each setting.

\section{Background}
\label{sec:back}

There is a large literature looking at various aspects of document classification, but only a few have focused on evaluating newspaper articles. Martin and Johnson (2015) used a dataset from the San Jose Mercury News and found that using a nouns-only or lemmatized version of the articles improved topic classification. However, theirs was an unsupervised model, so they evaluated their model using techniques that are not applicable in a supervised setting.

Meanwhile, Wermter and Hung (2002) created a semi-supervised self-organizing memory (SOM) model for topic generation of Reuters news articles. They used WordNet relationships to assist their model and thus limited their model input to only the nouns and verbs found in WordNet. This allowed them to achieve accuracies over 95\%, but generalizability may be limited due to this use of WordNet.

Other recent papers have looked at supervised topic classification for non-news documents. Karan et al (2016) looks at classification of Croatian political texts) using word2vec, logistic regression, Gaussian NB, and gradient-boosted trees, as well as a couple of postprocessing rules, to reach an F1 score of 77\% for major topics. Glava¨ et al (2017) looks at SVM and CNN models for multiple languages, and for monolingual English models finds that CNNs perform best with an accuracy of 57\%. In this paper, I will test whether some of the ideas and techniques used for classification of non-news documents can be successfully extended to the news article setting.

\section{Methods}
\label{sec:methods}

This section will discuss the New York Times dataset, the five model outputs, selection and cleaning of the model output variable, and the three classification models.

\subsection{Dataset}
\label{ssec:dataset}

For this analysis I used the New York Times Annotated Corpus, which contains 1.8 million articles from 1987-2007, labeled with topics, subtopics, and newspaper sections. Due to computing power limitations, I have selected a random sample of 100,000 articles to parse and use as data for the model. The articles are randomly split so that approximately 75\% are training data, 5\% are development data, and 20\% test data.

\subsection{Model Input}
\label{ssec:input}

I test the article's full text and four additional model inputs. Three are pulled from my review of the literature,  and I have added the fourth based on my understanding of the structure of news articles.

\begin{enumerate}
\item Full text of article
\item Article's lead paragraph, which is supposed to hook the reader, and in a news context often summarizes important details of the story \cite{NPR}
\item Article's headline \cite{Wermter:02}
\item Nouns-only version of the full text \cite{Martin:15}
\item Lemmatized version of the full text \cite{Martin:15}
\end{enumerate}

Table~\ref{tbl:inputs} shows some key statistics for each type of model input. The full text for the training articles (approximately 75,000 articles) adds up to about 50 million words and a vocabulary of over 300,000 words. The lemmas have roughly the same number of words, but much fewer unique words, with a vocabulary size 72\% of the full text vocabulary. There is a big drop down to the nouns, which have only 27\% of the total words that full text articles have, but 61\% of the vocabulary. As expected, headlines are the smallest category with less than a million words total, and the average individual headline having 8 words.

\begin{table}
	\centering
	\small
	%\makebox[\linewidth]{
	%\resizebox{\columnwidth}{!}{
	\begin{tabular}{| l | r r r |}
		\hline
		\multirow{2}{*}{\textbf{Model Input}} & \multicolumn{3}{c|}{\textbf{Training Words}} \\
		\cline{2-4}
		& \textbf{Count} & \textbf{\% Full Text} & \textbf{Avg. per Article} \\
		\hline
		Full Text			& 49.5M	& 100\%	& 668 \\
		Lead Paragraph	& 7.6M	& 15\%	& 105 \\
		Headlines			& 0.6M	& 1\%	& 8 \\
		Nouns			& 13.2M	& 27\%	& 178 \\
		Lemmas			& 50.4M	& 102\%	& 682 \\
		\hline
		\hline
		\multirow{2}{*}{\textbf{Model Input}} & \multicolumn{3}{c|}{\textbf{Training Vocab (Unique Words)}} \\
		\cline{2-4}
		& \textbf{Count} & \textbf{\% Full Text} & \textbf{Avg. per Article} \\
		\hline
		Full Text			& 327K	& 100	& 276 \\
		Lead Paragraph	& 140K	& 43		& 67 \\
		Headlines			& 42K	& 13		& 7 \\
		Nouns			& 198K	& 61		& 111 \\
		Lemmas			& 237K	& 72		& 253 \\
		\hline
	\end{tabular}
	%}
	\caption{Model Inputs}
	\label{tbl:inputs}
\end{table}

\subsection{Model Output}
\label{ssec:output}

The NYT corpus has multiple ways of classifying articles. Table~\ref{tbl:output} shows three examples from the data of the four different ways the articles were classified. In my models I use the \texttt{desk} column as my model output variable for two reasons:

\begin{enumerate}
\item It has the lowest percentage of null values (0.4\%)
\item It never assigns multiple descriptions to the same article
\end{enumerate}

\begin{table}
\centering
\footnotesize
%\makebox[\linewidth]{
\begin{tabular}{| llll |}
	\hline
	\textbf{Desk} & \textbf{General Descriptor} & \textbf{Online Sections} & \textbf{Taxonomic Classifier} \\
	\hline
	Foreign & \tabitem Immigration \& Refugees & \tabitem World & \tabitem Top/News/World/Europe \\
	Desk & \tabitem Jews & & \tabitem Top/News/World/Countries and Territories/Austria \\
	& \tabitem Music & & \tabitem Top/Features/Arts/Music \\
	& \tabitem Religion \& Churches & & \tabitem Etc. (8 others) \\
	\hline
	Book & \tabitem Books \& Literature & \tabitem Arts & \tabitem Top/Feature/Arts \\
	Review & & \tabitem Books & \tabitem Top/Features/Books \\
	Desk & & & \tabitem Top/Features/Books/Book Reviews \\
	\hline
	Classified & & \tabitem Paid Death Notices & \tabitem Top/Classifieds/Paid Death Notices \\
	\hline
\end{tabular}
%}
\caption{Possible Model Outputs}
\label{tbl:output}
\end{table}

However, \texttt{desk} still requires some clean-up before use due to misspellings and possible changes to the desk names over time. Table~\ref{tbl:label-clean} shows some examples of how I cleaned the \texttt{desk} variable so the model would have a standardized set of output labels for the articles. For example, there are significant clusters of articles around "Business/Financial Desk", "Financial Desk", and "Money and Business/Financial Desk" (plus misspellings of each of these), and these are all converted to have a single label of "business \& financial".

\begin{table}
\centering
%\small
\resizebox{\columnwidth}{!}{
\begin{tabular}{clr}
	\hline
	\textbf{Clean Label} & \textbf{Original Label} & \textbf{Count} \\
	\hline
	\multirow{2}{*}{book review} & Book Review Desk & 1,750 \\
	& Book Review Dest & 1 \\
	\hline
	& Business Desk & 4 \\
	& Business World Magazine & 8 \\
	& Business/Financial Desk & 6,078 \\
	& Business/Financial Desk; & 1 \\
	& Business/Financial desk & 1 \\
	& Business/FinancialDesk & 5 \\
	& Business\textbackslash Financial Desk & 2 \\
	business & E-Business & 6 \\
	\& & E-Commerce & 17 \\
	financial & Financial Desk & 11,276 \\
	& Financial Desk; & 16 \\
	& Money \& Business/Financial Desk & 3 \\
	& Money and Busines/Financial Desk & 1 \\
	& Money and Business/Financial Desk & 939 \\
	& Moneyand Business/Financial Desk & 1 \\
	& Small Business & 12 \\
	& SundayBusiness & 82 \\
	& The Business of Green & 3 \\
	\hline
	\multirow{2}{*}{cars} & Automobiles & 122 \\
	& Cars & 28 \\
	\hline
\end{tabular}
}
\caption{Label Cleaning Examples}
\label{tbl:label-clean}
\end{table}

After label cleaning, roughly 95\% of articles fall into the top 20 categories, and then there are a large number of very small categories with just a few articles. I therefore created an \texttt{<OTHER>} category as a catchall for these tiny categories, which individually don't have enough data for the models to learn very well. Figure~\ref{fig:topnhist} shows the frequency distribution of the top 20 category labels, plus \texttt{<OTHER>}. The top 8-10 categories are "usual suspects" for newspaper sections (e.g. business, sports, classifieds, editorials), and then we see some smaller categories like book reviews and a couple of weekly sections that are unique to the New York Times (New Jersey weekly, the city weekly, Westchester weekly, Long Island weekly).

\begin{figure}
\centering
\includegraphics[scale=0.55]{top_n_labels_histogram}
\caption{Frequency of Top 20 Labels + \small{\tt <OTHER>}}
\label{fig:topnhist}
\end{figure}

\subsection{Models}
\label{sec:models}

% For each set of training data, I TF-IDF vectorize it before feeding it into models.

For each model input, I tested two simple baseline models and a neural network:

\begin{enumerate}
\item Multinomial na\"{i}ve Bayes
\item Multi-class logistic regression (one vs. rest)
\item Convolutional neural network
\end{enumerate}

\subsubsection{MNB and LR}
\label{sssec:mnb-lr}

For the multinomial naive Bayes and logistic regression models, I TF-IDF vectorized the model input, dropping any words with a document frequency count of one. This significantly reduced the vocabulary sizes and got rid of a lot of misspellings and one-off words that just added extra noise to the models.

In each of these models I tested multiple parameter values on the dev data, and chose the parameter values that resulted in the highest accuracy. Figures~\ref{fig:mnb-acc} and~\ref{fig:lr-acc} show how the models performed with different parameter values, and reveal some interesting comparisons between the five model inputs that I will discuss further in Section~\ref{sec:results}.

\subsubsection{CNN}
\label{sssec:cnn}

When applying a convolutional neural network to this data, I found the main challenge was managing the size of the model, given how large the vocabularies are and how long some articles can be. 

\begin{enumerate}
\item Initialized the model with GloVe word embeddings \cite{GloVe}
\item Limited the vocabulary to the same size as the TF-IDF vocabulary (see Section~\ref{sssec:mnb-lr})
\item Padded each model input to the 90th percentile of lengths for that model input
\end{enumerate}

I chose this padding method because the length distribution for each model input is extremely right-skewed (e.g. the 90th percentile of full texts has 1,225 words, while the longest full text has over 9,000 words). It didn't make sense to pad model inputs out to the length of the longest article, since most articles are nowhere close to that length, and the model would be unnecessarily large. I therefore sacrificed the data at the ends of the longest articles in the hope of having a model I could actually train with the number of articles I am using.

\begin{figure}
\centering
\includegraphics[scale=0.55]{mnb_accuracy}
\caption{MNB Accuracy on Dev Data}
\label{fig:mnb-acc}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.55]{lr_accuracy}
\caption{LR Accuracy on Dev Data}
\label{fig:lr-acc}
\end{figure}

\section{Results \& Discussion}
\label{sec:results}

Note: \cite{Wermter:02} uses only top 8 topics.

\begin{table}
\centering
\small
\begin{tabular}{|l|rrr|p{1.2cm}|}
	\hline
	\multirow{2}{*}{\textbf{Model Input}} & \multicolumn{3}{c|}{\textbf{Model Type}} & \multicolumn{1}{c|}{\textbf{Best}} \\
	\cline{2-4}
	& \textbf{MNB} & \textbf{LR} & \textbf{CNN} & \multicolumn{1}{c|}{\textbf{Model}} \\
	\hline
	Full Text			& \textbf{0.729}	& \textbf{0.815}	& 0.190		& LR \\
	Lead Paragraph	& 0.682		& 0.731		& \textbf{0.248}	& LR \\
	Headlines			& 0.622		& 0.647		& 0.173		& LR \\
	Nouns			& 0.713		& 0.786		& 0.185		& LR \\
	Lemmas			& 0.727		& 0.811		& 0.189		& LR \\
	\hline
	\textbf{Best Input} & Full Text & Full Text & Lead Para & \textbf{Full Text in LR} \\
	\hline
\end{tabular}
\caption{Model Accuracies on Test Data}
\label{tbl:acc}
\end{table}

\begin{table}
\centering
\small
\begin{tabular}{lrrr}
	\textbf{Model Input} & \textbf{Vocab Size} & \textbf{Padding Size} \\
	\hline
	Full Text			& 207,070	& 500 \\
	Lead Paragraph	& 99,210	& 139 \\
	Headlines			& 30,658	& 11 \\
	Nouns			& 165,531	& 378 \\
	Lemmas			& 193,694	& 500 \\
\end{tabular}
\caption{CNN Vocabulary and Padding Sizes}
\label{tbl:cnn}
\end{table}

\section{Conclusion}
\label{sec:conc}

No time for: SVM, gradient boosted trees

An interesting future project would be to run an unsupervised topic classification algorithm on this corpus (e.g. \citeauthor{Martin:15}~\shortcite{Martin:15} use the Latent Dirichlet Allocation algorithm) and compare those results to the supervised learning results.

\bibliography{final-report} % references .bib file
\bibliographystyle{acl_natbib} % references acl_natbib.bst

\end{document}
